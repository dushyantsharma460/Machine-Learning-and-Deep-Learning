{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d28c18e3-89eb-4d8f-b1de-27594b836c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-10 18:53:25.868287: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-09-10 18:53:25.868528: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-09-10 18:53:25.903161: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-09-10 18:53:26.724140: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-09-10 18:53:26.724414: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Input\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical   # For one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2562b460-c230-4a90-a47d-0842f2e7055f",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "573dc9ba-fba2-4886-83ce-0fcebe8c4ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000,)\n",
      "[5 0 4 ... 5 6 8]\n"
     ]
    }
   ],
   "source": [
    "# x_train[i] = the i-th image\n",
    "# y_train[i] = the correct digit for that image\n",
    "\n",
    "# x_train[0]\n",
    "\n",
    "# y_train looks like [5, 0, 4, 1, 9, ...]\n",
    "print(y_train.shape)\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b96494a9-3212-484a-b20b-f721f744068d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature scaling\n",
    "\n",
    "# ğŸ‘‰ The pixel values in MNIST images range from 0 to 255 (since each pixel is stored as an 8-bit integer).\n",
    "# 0 = black\n",
    "# 255 = white\n",
    "# Values in between = shades of gray\n",
    "\n",
    "x_train = x_train/255.0\n",
    "x_test = x_test/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d4eff52-bf10-4861-899a-1c64756ff92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After normalizing this x_train\n",
    "# x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d2cc051-d672-4a1c-ba3b-873b947ff6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encoder to labels\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08cdd034-9927-4a67-adad-3135b06c8169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 10)\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# â†’ 60,000 labels, but each label is now a vector of length 10.\n",
    "# 5 â†’ [0,0,0,0,0,1,0,0,0,0]\n",
    "print(y_train.shape)\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "185339f9-624f-4370-95cc-f7023c1da55d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-10 18:53:27.758302: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([                           # Create a Sequential model (layers stacked one after another)\n",
    "    Input(shape=(28, 28)),                     # Input layer: expects 28x28 grayscale images\n",
    "    Flatten(),                                 # Flattens 28x28 into 784 values (1D vector)\n",
    "    Dense(128, activation=\"relu\"),             # Hidden layer: 128 neurons, ReLU adds non-linearity\n",
    "    Dense(10, activation=\"softmax\")            # Output layer: 10 neurons for digits 0â€“9, softmax gives probabilities\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91d43c0e-24e1-4c0b-bafe-587c9e5f2c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=\"adam\",                          # Optimizer: Adam adjusts weights during training efficiently\n",
    "    loss=\"categorical_crossentropy\",           # Loss function: compares predicted probabilities vs actual labels\n",
    "    metrics=['accuracy']                       # Metric: track accuracy while training/testing\n",
    ")\n",
    "\n",
    "# This adjustment is done using an optimizer.\n",
    "# Adam is one of the most popular optimizers.\n",
    "# It automatically decides how big or small the weight updates should be for each neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d9eff27-dbff-42dc-b0c0-a0667088cab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m3750/3750\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - accuracy: 0.9350 - loss: 0.2284\n",
      "Epoch 2/5\n",
      "\u001b[1m3750/3750\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - accuracy: 0.9700 - loss: 0.0995\n",
      "Epoch 3/5\n",
      "\u001b[1m3750/3750\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - accuracy: 0.9789 - loss: 0.0692\n",
      "Epoch 4/5\n",
      "\u001b[1m3750/3750\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - accuracy: 0.9839 - loss: 0.0510\n",
      "Epoch 5/5\n",
      "\u001b[1m3750/3750\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - accuracy: 0.9880 - loss: 0.0381\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x745ae80d06e0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=5, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a5582d5-2785-4e00-bb52-4784389ee0e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9763 - loss: 0.0808   \n",
      "The test accuracy is 0.9763000011444092\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print(f\"The test accuracy is {test_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8249bdd7-46ac-49f9-9468-139fd4f4be53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "Predicted digit: 6\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def preprocess_image(img_path):\n",
    "    # Read as grayscale\n",
    "    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "    \n",
    "    # Invert if background is white\n",
    "    if np.mean(img) > 127:\n",
    "        img = cv2.bitwise_not(img)\n",
    "    \n",
    "    # Threshold the image (binary)\n",
    "    _, img = cv2.threshold(img, 128, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    # Find contours to crop the digit\n",
    "    contours, _ = cv2.findContours(img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if contours:\n",
    "        x, y, w, h = cv2.boundingRect(contours[0])\n",
    "        img = img[y:y+h, x:x+w]\n",
    "    \n",
    "    # Resize while keeping aspect ratio\n",
    "    h, w = img.shape\n",
    "    if h > w:\n",
    "        new_h = 20\n",
    "        new_w = int(w * (20/h))\n",
    "    else:\n",
    "        new_w = 20\n",
    "        new_h = int(h * (20/w))\n",
    "    img = cv2.resize(img, (new_w, new_h))\n",
    "    \n",
    "    # Pad to 28x28\n",
    "    pad_top = (28 - new_h) // 2\n",
    "    pad_bottom = 28 - new_h - pad_top\n",
    "    pad_left = (28 - new_w) // 2\n",
    "    pad_right = 28 - new_w - pad_left\n",
    "    img = cv2.copyMakeBorder(img, pad_top, pad_bottom, pad_left, pad_right,\n",
    "                             cv2.BORDER_CONSTANT, value=0)\n",
    "    \n",
    "    # Normalize\n",
    "    img = img / 255.0\n",
    "    img = np.expand_dims(img, axis=0)   # shape (1, 28, 28)\n",
    "    \n",
    "    return img\n",
    "\n",
    "# Use it\n",
    "img = preprocess_image(\"6_image.png\")\n",
    "prediction = model.predict(img)\n",
    "predicted_class = np.argmax(prediction)\n",
    "print(\"Predicted digit:\", predicted_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1e0f1ba-7939-407e-9bc9-a90c974fbcf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Predicted digit: 2\n"
     ]
    }
   ],
   "source": [
    "# Correct only for MNIST-like images with proper padding like the MNIST dataset.\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img_path = \"2_image.png\"  # your file\n",
    "\n",
    "# Read as grayscale\n",
    "img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Invert colors: black digit â†’ white digit\n",
    "img = cv2.bitwise_not(img)\n",
    "\n",
    "# Resize to 28x28 (like MNIST)\n",
    "img = cv2.resize(img, (28, 28))\n",
    "\n",
    "# Normalize\n",
    "img = img / 255.0\n",
    "\n",
    "# Reshape for model input\n",
    "img = np.expand_dims(img, axis=0)  # shape (1, 28, 28)\n",
    "\n",
    "# Predict\n",
    "prediction = model.predict(img)\n",
    "predicted_class = np.argmax(prediction)\n",
    "\n",
    "print(\"Predicted digit:\", predicted_class)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
